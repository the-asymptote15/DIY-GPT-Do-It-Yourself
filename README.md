# ğŸš€ From Bi-grams to GPT: A Journey in Language Modeling

Welcome to **From Bi-grams to GPT**, a repository that explores the evolution of language modelsâ€”starting from a simple statistical bigram model and progressing to a fully functional GPT-style transformer model, all implemented from scratch in PyTorch!

## ğŸ“‚ Project Structure

```
ğŸ“¦ Your-Repository-Name
â”£ ğŸ“œ gpt.py                 # A transformer-based language model (GPT-like)
â”£ ğŸ“œ statistical_bigram.py   # A simple statistical bigram model
â”£ ğŸ“œ input.txt               # Training dataset (Shakespeare text)
â”£ ğŸ“œ README.md               # You're reading this! ğŸ˜Š
```

## ğŸ§  What's Inside?

### 1ï¸âƒ£ **Bigram Model (**\`\`**)**

ğŸ”¹ A simple statistical approach where the next token is predicted based on the previous one.\
ğŸ”¹ Uses an embedding table where each token learns its next-token probabilities.\
ğŸ”¹ Quick to train but lacks deep contextual understanding.

### 2ï¸âƒ£ **GPT-like Transformer (**\`\`**)**

ğŸš€ A modern language model with:\
âœ” Multi-head self-attention for context awareness\
âœ” Position embeddings to capture sequential order\
âœ” Transformer blocks with layer normalization and dropout\
âœ” Token generation using learned probabilities

## ğŸ”§ Setup

1. **Clone the repo**

   ```bash
   git clone https://github.com/your-username/your-repository-name.git
   cd your-repository-name
   ```

2. **Install dependencies**

   ```bash
   pip install torch
   ```

3. **Train and Run the Models**

   - To train the **Bigram Model**:
     ```bash
     python statistical_bigram.py
     ```
   - To train the **GPT Model**:
     ```bash
     python gpt.py
     ```

## ğŸ“Š Model Comparison

| Model              | Context Length | Parameters | Training Time | Output Quality           |
| ------------------ | -------------- | ---------- | ------------- | ------------------------ |
| **Bigram Model**   | 1 token        | Very Few   | âš¡ Super Fast  | Basic word prediction    |
| **GPT-like Model** | 256 tokens     | Millions   | ğŸ•’ Slower     | Coherent text generation |

## ğŸ¯ Example Generated Output

After training, the GPT model can generate text like:

> *"To be or not to be, that is the question, whether 'tis nobler in the mind to suffer..."*

## ğŸ¤– Future Enhancements

- Train on larger datasets

- Implement a more advanced transformer architecture

- Experiment with different tokenization strategies



---

ğŸ’¡ **Inspired by Karpathyâ€™s nanoGPT and transformer models, this project helps build an intuition for modern NLP.**

ğŸ“¢ If you find this useful, donâ€™t forget to â­ **star this repo** and contribute! ğŸš€

