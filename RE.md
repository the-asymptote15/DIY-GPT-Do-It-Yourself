# ğŸš€ From Bi-grams to GPT: A Journey in Language Modeling

Welcome to **From Bi-grams to GPT**, a repository that explores the evolution of language modelsâ€”starting from a simple statistical bigram model and progressing to a fully functional GPT-style transformer model, all implemented from scratch in PyTorch!  

## ğŸ“‚ Project Structure  


## ğŸ§  What's Inside?  

### 1ï¸âƒ£ **Bigram Model (`statistical_bigram.py`)**  
ğŸ”¹ A simple statistical approach where the next token is predicted based on the previous one.  
ğŸ”¹ Uses an embedding table where each token learns its next-token probabilities.  
ğŸ”¹ Quick to train but lacks deep contextual understanding.  

### 2ï¸âƒ£ **GPT-like Transformer (`gpt.py`)**  
ğŸš€ A modern language model with:  
âœ” Multi-head self-attention for context awareness  
âœ” Position embeddings to capture sequential order  
âœ” Transformer blocks with layer normalization and dropout  
âœ” Token generation using learned probabilities  

## ğŸ”§ Setup  

1. **Clone the repo**  
   ```bash
   git clone https://github.com/your-username/your-repository-name.git
   cd your-repository-name

2. **Install dependencies**  

pip install torch


3. **Train and Run the Models**  

python statistical_bigram.py

